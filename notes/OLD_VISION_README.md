
<img src="static/resources/logo-nobg.png" alt="Joulin Logo" align="center" width="200">

# Shippingâ€‘QC Vision System

A hybrid, localâ€‘first computerâ€‘vision workflow for verifying outgoing orders on the shipping line.  
The system prompts operators for required photos, automatically counts parts, and stores images
together with order metadata.

---

## App Structure
main.js           â†’ Application controller & event coordination
hud-manager.js    â†’ All HUD/UI management 
camera.js         â†’ Camera operations & photo capture
photo-queue.js    â†’ Data persistence & management
ux-elements.js    â†’ Just modal & utility functions

## ğŸ“‹ Function Call Flow:
User clicks shutter â†’ main.js calls capturePhoto() 
â†’ camera.js captures & dispatches PHOTO_CAPTURED event
â†’ main.js event listener calls hudManager.onPhotoCaptured()
â†’ HUD updates photos and item counts

## ğŸ”„ Complete Cycle:
Scan â†’ HUD opens â†’ Take photos/check items â†’ Click Done
â†’ Validation passes â†’ Upload â†’ Clear session 
â†’ while loop continues â†’ Modal shows "Waiting for scan..."
â†’ Ready for next barcode

## âœ… Client-Side Logging Strategy
I implemented a multi-tier posting strategy that ensures reliable log delivery:
ğŸ“Š Posting Strategy:

ğŸš¨ IMMEDIATE: Critical errors sent instantly
â° BATCH: Regular logs sent every 30 seconds
ğŸ“‹ SESSION: Complete session uploaded on "Done" click
ğŸ”„ VISIBILITY: Logs flushed when user switches apps/tabs
ğŸ’¾ BEACON: Final logs sent during page unload (most reliable)

# ğŸ“ˆ Benefits:

Reliable delivery: Multiple fallback mechanisms
Performance: Batched sending reduces server load
Completeness: Captures logs even if user closes browser unexpectedly
Smart: Only sends when necessary, queues when offline

## Architecture at a Glance

| Layer | Technology | Purpose |
|-------|------------|---------|
| **Local detector** | **YOLOv9â€‘Seg** exported to CoreÂ ML | Realâ€‘time instance masks & counts on iPad (<â€¯10â€¯ms @â€¯640Ã—640). |
| **Openâ€‘set fallback** | **GroundingÂ DINOÂ 1.5 â†’ SAMÂ 2** | Zeroâ€‘shot detection + pixelâ€‘perfect masks on Vercel GPU. |
| **Ultraâ€‘dense counter** | **CSRNet** | Densityâ€‘map counting when objects are piled densely. |
| **Frontâ€‘end** | SwiftUI (production) & Next.jsÂ 14 (browser demo) | Works on shopâ€‘floor iPads *and* any phone for quick testing. |
| **Backend / API** | FastAPI + TorchServe inâ€¯Docker | Heavy models & async jobs. |
| **Storage** | S3 (MinIO/AWS) + PostgreSQL | Images in object storage, order â†” photo rows in DB. |

---

## Objectives

1. **Photo compliance** â€“ prompt and verify all required views.
2. **Automated counting** â€“ instant onâ€‘device count; cloud fallback for hard scenes.
3. **Order association** â€“ barcode â†’ ERP â†’ expected SKU list; store `/customer_id/order_id/*.jpg` + JSON.

---

## Data & Training

| Model | Input | Labels | Reason |
|-------|-------|--------|--------|
| YOLOv9â€‘Seg | 640Ã—640 RGB, heavy augment | Polygon masks / instance IDs | Fast; excels at small objects. |
| GroundingÂ DINOÂ 1.5 | Image + text prompt | None (zeroâ€‘shot) | Handles unseen SKUs without retraining. |
| SAMÂ 2 | DINO box â†’ SAM | None | Sharpens masks for accurate count. |
| CSRNet | 512Ã—512 crops | Density maps | Use when parts form nearâ€‘continuous blobs. |

### Activeâ€‘Learning Loop

1. YOLO runs on device.  
2. Frames with confidenceÂ <Â 0.5 autoâ€‘upload for review.  
3. Human labels only these; nightly retrain; push new CoreÂ ML OTA.

---

## Repository Layout

```
app/
â”œâ”€â”€ packages/
â”‚   â”œâ”€â”€ models/            # ONNX / CoreÂ ML binaries
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ yolo/          # Next.jsÂ API route â†’ WASM YOLO
â”‚       â””â”€â”€ fallback/      # GPU function calling TorchServe
â”œâ”€â”€ prisma/                # PostgreSQL schema
â”œâ”€â”€ public/                # Static assets
â””â”€â”€ README.md              # (this file)
```

---

## QuickÂ Start (browser demo on Vercel)

```bash
# 1Â Â·Â Create project
npx create-next-app vision-demo --typescript --app
cd vision-demo
npm i ultralytics-web @aws-sdk/client-s3 @aws-sdk/s3-request-presigner

# 2Â Â·Â Add API route `app/api/infer/route.ts`
#     â€“ runs WASM YOLO, falls back to GPU function

# 3Â Â·Â Set env vars in Vercel dashboard:
#     DATABASE_URL  S3_ENDPOINT  S3_KEY  S3_SECRET  ERP_API_KEY

git init && git add . && git commit -m "init"
vercel --prod
# Open https://<yourâ€‘app>.vercel.app on your phone
```

---

## TrainingÂ Scripts

### YOLOv9â€‘Seg Fineâ€‘Tuning

```bash
pip install ultralytics
yolo task=segment mode=train      model=yolov9n-seg.pt      data=data/parts.yaml      imgsz=640 batch=32 epochs=60      lr0=5e-4 project=models/parts_run

# Export to CoreÂ ML
yolo mode=export format=coreml      model=models/parts_run/weights/best.pt      imgsz=640
```

### GroundingÂ DINOÂ 1.5 + SAMÂ 2 Server

```bash
git clone https://github.com/IDEA-Research/GroundingDINO.git
cd GroundingDINO
pip install -r requirements.txt
python demo/grd_sam2_server.py        --dino_ckpt weights/gdino1.5_pro.pth        --sam_ckpt  weights/sam2.pth        --device cuda
# Package with TorchServe or run as a longâ€‘living GPU function.
```

---

## NextÂ Steps

* Integrate barcode â†’ ERP lookup in the demo so it knows expected counts.  
* Record lowâ€‘confidence inferences to a â€œtoâ€‘labelâ€ queue (LabelÂ Studio).  
* Port React camera UI to SwiftUI for production iPad app.
